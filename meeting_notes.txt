    # add a way to get the number of functions being 
    # and print it out in the end of the program, maybe create a hastable that stores the function with the call time and function count
    # each time its called, add time, or record each time its called. 
    # in the hash table, we know how many times its called, and total time, STDDEV, and highest / lowest time that the function is executed
    # key is function name, value is time and counter

    # try to find a way to separate the workload from the profiler, so that we can run the workload multiple times
    # and average the results. no neeed to change the tool every time we run the workload.
    # (1/6)try a different approach with line numbers, try to use a different identifier when working with mulitple files.
    # how to measure overhead, and see how it differs between other data structures.
    # maybe use a different approach to measure time, like time.perf_counter() or time.process_time()
    # hashtable should not have string as key, needs to be unqieu function identifier, string is not efficient. maybe use integer number, 
    # maybe can use array or other data strcuture to make it more efficient.
    # if we need to connect the profiling with LTTNG since itd in c, then we need to use sys mon, we need to go through the process of calling the functins through c
    # first we need to decide this approach, save a lot of time to use a pyeval with LTTNG-ust
    next step: try to use cprofile, rebuild it so we can use the profiler. create another version of cprofiler and rebuild the runtime to see what we want. create a new version of python command so we can use what we want. this is bascailly a shared library file. 
    clone the cprofile repo, find the function that is relevant to function calls, and add some print functions so we can get an idea of how its working. 

    1/12
    cloned the repo and tried a new "hacked" version of cpython and created a VENV for it. it takes about a minute for it to process and actually work, but it does work.
    (i highly recommend putting these in the same mother/root directory) 

    
    navigate to the python-sys.monitoring directory,
    cd ~/cProfiler/python-sys.monitoring

    create a new VENV with the new cpython 
    ../cpython/python -m venv .venv

    activate it and check version, should be on 3.12 / 3.13 / 3.15
    cd ~/cProfiler/python-sys.monitoring
    source .venv/bin/activate
    python -V

    install dependencies
    pip install --upgrade pip
    pip install numpy
    pip install torch --index-url https://download.pytorch.org/whl/cpu

    run files

    raw tracer:
    python tracer.py

    sysmon profiler:
    python sysmon_profiler.py

    baseline cProfile inside VENV
    python -m cProfile -o cprofile_stats.dat workload.py
    python -m pstats cprofile_stats.dat
    sort cumulative
    stats 20
    quit

    created a new branch sysmon-cprofile-experiment
    this has the cProfiler patch i need to make this work.

    issues when approaching this:
    I had an environment mismatch which broke the system when trying to implement. 
    i had to learn how to use patches so i dont have the cpython library with every file.
    I had to learn how to configure files to  

    **GENERATED STEPS**
    ### Patched CPython for cProfile/sys.monitoring

These experiments require a patched CPython so that `cProfile` uses
our `sys.monitoring` hooks.

Steps:

```bash
# 1) Clone CPython at a supported version
git clone https://github.com/python/cpython.git
cd cpython
git checkout v3.12.7   # or whatever tag is needed, 3.12/3.13/3.15

# 2) Apply our patch
git apply /path/to/python-sys.monitoring/patches/cpython_lsprof_sysmon.patch

# 3) Build
./configure
make -j8

# 4) Use the patched interpreter
./python -m cProfile -o /tmp/hacked_cprofile_stats.dat \
    /path/to/python-sys.monitoring/workload.py


